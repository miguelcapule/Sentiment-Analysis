{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b510a61",
   "metadata": {},
   "source": [
    "# Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3b199862",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install matplotlib pandas nltk textblob "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10abcfda",
   "metadata": {},
   "source": [
    "# NLTK: Natural Language Tooklit\n",
    "\n",
    "Natural Language Toolkit is the basis for a lot of text analysis done in Python. It's old and terrible and slow, but it's just been used for so long and does so many things that it's generally the default when people get into text analysis. The new kid on the block is spaCy (but it doesn't do sentiment analysis out of the box so we're leaving it out of this).\n",
    "\n",
    "When you first run NLTK, you need to download some datasets to make sure it will be able to do everything you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "361c38e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to C:\\Users\\Miguel\n",
      "[nltk_data]     Capule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package movie_reviews to C:\\Users\\Miguel\n",
      "[nltk_data]     Capule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package movie_reviews is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\Miguel\n",
      "[nltk_data]     Capule\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "nltk.download('movie_reviews')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "306c7269",
   "metadata": {},
   "source": [
    "To do sentiment analysis with NLTK, it only takes a couple lines of code. To determine sentiment, it's using a tool called VADER."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ba26dd2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.153, 'neu': 0.688, 'pos': 0.159, 'compound': 0.0276}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer as SIA\n",
    "\n",
    "sia = SIA()\n",
    "sia.polarity_scores(\"This restaurant was great, but I'm not sure if I'll go there again.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc2b377",
   "metadata": {},
   "source": [
    "Asking SentimentIntensityAnalyzer for the polarity_score gave us four values in a dictionary:\n",
    "\n",
    "    - negative: the negative sentiment in a sentence\n",
    "\n",
    "    - neutral: the neutral sentiment in a sentence\n",
    "\n",
    "    - positive: the postivie sentiment in the sentence\n",
    "\n",
    "    - compound: the aggregated sentiment.\n",
    "        \n",
    "Seems simple enough!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "676ca960",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday?\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04322c42",
   "metadata": {},
   "source": [
    "Just like in real life, if you use an emoticon you can be read as being more positive:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a9a8b87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 0.786, 'pos': 0.214, 'compound': 0.4588}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday? :)\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b1287e0",
   "metadata": {},
   "source": [
    "But what if we swap out the emoticon for an emoji?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "35dccd1b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'neg': 0.0, 'neu': 1.0, 'pos': 0.0, 'compound': 0.0}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"I just got a call from my boss - does he realise it's Saturday? ðŸ˜Š\"\n",
    "sia.polarity_scores(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de8fffee",
   "metadata": {},
   "source": [
    "Back to neutral! Why didn't it understand the emoji the same way it understood the emoticon? Well, text analysis tools only knows the words that they've been taught, and if VADER's never seen ðŸ˜Š before it won't know what to think of it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0e735f9",
   "metadata": {},
   "source": [
    "# TextBlob\n",
    "\n",
    "TextBlob is built on top of NLTK, but is infinitely easier to use. It's still slow, but it's so so so easy to use.\n",
    "\n",
    "You can just feed TextBlob your sentence, then ask for a .sentiment!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7cd7ab95",
   "metadata": {},
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Blobber\n",
    "from textblob.sentiments import NaiveBayesAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98eb59c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(polarity=0.275, subjectivity=0.8194444444444444)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blob = TextBlob(\"This restaurant was great, but I'm not sure if I'll go there again.\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6868a7f",
   "metadata": {},
   "source": [
    "How could it possibly be easier than that?!?!? This time we get a polarity and a subjectivity instead of all of those different scores, but it's basically the same idea.\n",
    "\n",
    "If you like options: it turns out TextBlob actually has multiple sentiment analysis tools! How fun! We can plug in a different analyzer to get a different result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b5fef1b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sentiment(classification='pos', p_pos=0.5879425317005774, p_neg=0.41205746829942275)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blobber = Blobber(analyzer=NaiveBayesAnalyzer())\n",
    "\n",
    "blob = blobber(\"This restaurant was great, but I'm not sure if I'll go there again.\")\n",
    "blob.sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9d4143b",
   "metadata": {},
   "source": [
    "Wow, that's a very different result. To understand why it's so different, we need to talk about where these sentiment numbers come from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19aebfca",
   "metadata": {},
   "source": [
    "# But where do those numbers come from?\n",
    "\n",
    "The most important thing to understand is sentiment is always just an opinion. In this case it's an opinion, yes, but specifically the opinion of a machine.\n",
    "\n",
    "# VADER\n",
    "\n",
    "NLTK's Sentiment Intensity Analyzer works is using something called VADER, which is a list of words that have a sentiment associated with each of them.\n",
    "\n",
    " Word\t   Sentiment rating\n",
    "tragedy\t       - 3.4\n",
    "rejoiced       - 2.0\n",
    "disaster       - 3.1\n",
    "great          - 3.1\n",
    "\n",
    "If you have more positives, the sentence is more positive. If you have more negatives, it's more negative. It can also take into account things like capitalization - you can read more about the classifier here, or the actual paper it came out of here.\n",
    "\n",
    "How do they know what's positive/negative? They came up with a very big list of words, then asked people on the internet and paid them one cent for each word they scored.\n",
    "\n",
    "# TextBlob's .sentiment\n",
    "\n",
    "TextBlob's sentiment analysis is based on a separate library called pattern.\n",
    "\n",
    "- The sentiment analysis lexicon bundled in Pattern focuses on adjectives. It contains adjectives that occur frequently in - - customer reviews, hand-tagged with values for polarity and subjectivity.\n",
    "\n",
    "Same kind of thing as NLTK's VADER, but it specifically looks at words from customer reviews.\n",
    "\n",
    "How do they know what's positive/negative? They look at (mostly) adjectives that occur in customer reviews and hand-tag them.\n",
    "\n",
    "# TextBlob's .sentiment + NaiveBayesAnalyzer\n",
    "\n",
    "TextBlob's other option uses a NaiveBayesAnalyzer, which is a machine learning technique. When you use this option with TextBlob, the sentiment is coming from \"an NLTK classifier trained on a movie reviews corpus.\"\n",
    "\n",
    "How do they know what's positive/negative? Looked at movie reviews and scores using machine learning, the computer automatically learned what words are associated with a positive or negative rating.\n",
    "\n",
    "# What's this mean for me?\n",
    "\n",
    "When you're doing sentiment analysis with tools like this, you should have a few major questions:\n",
    "\n",
    "    - Where kind of dataset does the list of known words come from?\n",
    "    - Do they use all the words, or a selection of the words?\n",
    "    - Where do the positive/negative scores come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9df6ec5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I love love love love this kitten</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I hate hate hate hate this keyboard</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I'm not sure how I feel about toast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Did you see the baseball game yesterday?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The package was delivered late and the contents were broken</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Trashy television shows are some of my favorites</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>I find chirping birds irritating, but I know I'm not the only one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                                    content\n",
       "0                                         I love love love love this kitten\n",
       "1                                       I hate hate hate hate this keyboard\n",
       "2                                       I'm not sure how I feel about toast\n",
       "3                                  Did you see the baseball game yesterday?\n",
       "4               The package was delivered late and the contents were broken\n",
       "5                          Trashy television shows are some of my favorites\n",
       "6  I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\n",
       "7         I find chirping birds irritating, but I know I'm not the only one"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.set_option(\"display.max_colwidth\", 200)\n",
    "\n",
    "df = pd.DataFrame({'content': [\n",
    "    \"I love love love love this kitten\",\n",
    "    \"I hate hate hate hate this keyboard\",\n",
    "    \"I'm not sure how I feel about toast\",\n",
    "    \"Did you see the baseball game yesterday?\",\n",
    "    \"The package was delivered late and the contents were broken\",\n",
    "    \"Trashy television shows are some of my favorites\",\n",
    "    \"I'm seeing a Kubrick film tomorrow, I hear not so great things about it.\",\n",
    "    \"I find chirping birds irritating, but I know I'm not the only one\",\n",
    "]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6c5784f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "#T_017bc_row0_col1 {\n",
       "  background-color: #c3e67d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row0_col2 {\n",
       "  background-color: #fff6b0;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row0_col3 {\n",
       "  background-color: #73c264;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row1_col1 {\n",
       "  background-color: #fa9656;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row1_col2 {\n",
       "  background-color: #feeb9d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row1_col3 {\n",
       "  background-color: #f67a49;\n",
       "  color: #f1f1f1;\n",
       "}\n",
       "#T_017bc_row2_col1, #T_017bc_row7_col3 {\n",
       "  background-color: #fee797;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row2_col2 {\n",
       "  background-color: #d3ec87;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row2_col3 {\n",
       "  background-color: #fee999;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row3_col1 {\n",
       "  background-color: #fed683;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row3_col2 {\n",
       "  background-color: #b1de71;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row3_col3, #T_017bc_row5_col1 {\n",
       "  background-color: #fffebe;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row4_col1 {\n",
       "  background-color: #fede89;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row4_col2 {\n",
       "  background-color: #fdbd6d;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row4_col3 {\n",
       "  background-color: #feca79;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row5_col2 {\n",
       "  background-color: #fbfdba;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row5_col3 {\n",
       "  background-color: #cfeb85;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row6_col1 {\n",
       "  background-color: #91d068;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row6_col2 {\n",
       "  background-color: #a0d669;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row6_col3 {\n",
       "  background-color: #fdb567;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row7_col1 {\n",
       "  background-color: #feec9f;\n",
       "  color: #000000;\n",
       "}\n",
       "#T_017bc_row7_col2 {\n",
       "  background-color: #e3f399;\n",
       "  color: #000000;\n",
       "}\n",
       "</style>\n",
       "<table id=\"T_017bc\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_017bc_level0_col0\" class=\"col_heading level0 col0\" >content</th>\n",
       "      <th id=\"T_017bc_level0_col1\" class=\"col_heading level0 col1\" >textblob</th>\n",
       "      <th id=\"T_017bc_level0_col2\" class=\"col_heading level0 col2\" >textblob_bayes</th>\n",
       "      <th id=\"T_017bc_level0_col3\" class=\"col_heading level0 col3\" >nltk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row0\" class=\"row_heading level0 row0\" >0</th>\n",
       "      <td id=\"T_017bc_row0_col0\" class=\"data row0 col0\" >I love love love love this kitten</td>\n",
       "      <td id=\"T_017bc_row0_col1\" class=\"data row0 col1\" >0.500000</td>\n",
       "      <td id=\"T_017bc_row0_col2\" class=\"data row0 col2\" >-0.087933</td>\n",
       "      <td id=\"T_017bc_row0_col3\" class=\"data row0 col3\" >0.957100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row1\" class=\"row_heading level0 row1\" >1</th>\n",
       "      <td id=\"T_017bc_row1_col0\" class=\"data row1 col0\" >I hate hate hate hate this keyboard</td>\n",
       "      <td id=\"T_017bc_row1_col1\" class=\"data row1 col1\" >-0.800000</td>\n",
       "      <td id=\"T_017bc_row1_col2\" class=\"data row1 col2\" >-0.214151</td>\n",
       "      <td id=\"T_017bc_row1_col3\" class=\"data row1 col3\" >-0.941300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row2\" class=\"row_heading level0 row2\" >2</th>\n",
       "      <td id=\"T_017bc_row2_col0\" class=\"data row2 col0\" >I'm not sure how I feel about toast</td>\n",
       "      <td id=\"T_017bc_row2_col1\" class=\"data row2 col1\" >-0.250000</td>\n",
       "      <td id=\"T_017bc_row2_col2\" class=\"data row2 col2\" >0.394659</td>\n",
       "      <td id=\"T_017bc_row2_col3\" class=\"data row2 col3\" >-0.241100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row3\" class=\"row_heading level0 row3\" >3</th>\n",
       "      <td id=\"T_017bc_row3_col0\" class=\"data row3 col0\" >Did you see the baseball game yesterday?</td>\n",
       "      <td id=\"T_017bc_row3_col1\" class=\"data row3 col1\" >-0.400000</td>\n",
       "      <td id=\"T_017bc_row3_col2\" class=\"data row3 col2\" >0.613050</td>\n",
       "      <td id=\"T_017bc_row3_col3\" class=\"data row3 col3\" >0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row4\" class=\"row_heading level0 row4\" >4</th>\n",
       "      <td id=\"T_017bc_row4_col0\" class=\"data row4 col0\" >The package was delivered late and the contents were broken</td>\n",
       "      <td id=\"T_017bc_row4_col1\" class=\"data row4 col1\" >-0.350000</td>\n",
       "      <td id=\"T_017bc_row4_col2\" class=\"data row4 col2\" >-0.574270</td>\n",
       "      <td id=\"T_017bc_row4_col3\" class=\"data row4 col3\" >-0.476700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row5\" class=\"row_heading level0 row5\" >5</th>\n",
       "      <td id=\"T_017bc_row5_col0\" class=\"data row5 col0\" >Trashy television shows are some of my favorites</td>\n",
       "      <td id=\"T_017bc_row5_col1\" class=\"data row5 col1\" >0.000000</td>\n",
       "      <td id=\"T_017bc_row5_col2\" class=\"data row5 col2\" >0.040076</td>\n",
       "      <td id=\"T_017bc_row5_col3\" class=\"data row5 col3\" >0.421500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row6\" class=\"row_heading level0 row6\" >6</th>\n",
       "      <td id=\"T_017bc_row6_col0\" class=\"data row6 col0\" >I'm seeing a Kubrick film tomorrow, I hear not so great things about it.</td>\n",
       "      <td id=\"T_017bc_row6_col1\" class=\"data row6 col1\" >0.800000</td>\n",
       "      <td id=\"T_017bc_row6_col2\" class=\"data row6 col2\" >0.717875</td>\n",
       "      <td id=\"T_017bc_row6_col3\" class=\"data row6 col3\" >-0.629600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_017bc_level0_row7\" class=\"row_heading level0 row7\" >7</th>\n",
       "      <td id=\"T_017bc_row7_col0\" class=\"data row7 col0\" >I find chirping birds irritating, but I know I'm not the only one</td>\n",
       "      <td id=\"T_017bc_row7_col1\" class=\"data row7 col1\" >-0.200000</td>\n",
       "      <td id=\"T_017bc_row7_col2\" class=\"data row7 col2\" >0.257148</td>\n",
       "      <td id=\"T_017bc_row7_col3\" class=\"data row7 col3\" >-0.250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x207b111b190>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_scores(content):\n",
    "    blob = TextBlob(content)\n",
    "    nb_blob = blobber(content)\n",
    "    sia_scores = sia.polarity_scores(content)\n",
    "    \n",
    "    return pd.Series({\n",
    "        'content': content,\n",
    "        'textblob': blob.sentiment.polarity,\n",
    "        'textblob_bayes': nb_blob.sentiment.p_pos - nb_blob.sentiment.p_neg,\n",
    "        'nltk': sia_scores['compound'],\n",
    "    })\n",
    "\n",
    "scores = df.content.apply(get_scores)\n",
    "scores.style.background_gradient(cmap='RdYlGn', axis=None, low=0.4, high=0.4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd3e81e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
